{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd176c9-71b0-4ebf-99ea-772772e3e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b4e929-353b-494a-92cf-add130aa3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[\n",
    "    \"A comida estava deliciosa e o atendimento foi impecável.\",\n",
    "    \"Esperei mais de 40 minutos pelo prato principal e veio frio.\",\n",
    "    \"O ambiente é acolhedor, ideal para um jantar romântico.\",\n",
    "    \"Achei os preços altos para a qualidade do que foi servido.\",\n",
    "    \"Melhor risoto que já comi, voltarei com certeza!\",\n",
    "    \"Garçom atencioso, mas a cozinha demorou muito.\",\n",
    "    \"Cardápio criativo e pratos bem apresentados.\",\n",
    "    \"Poucas opções vegetarianas e nenhuma sobremesa vegana.\",\n",
    "    \"Atendimento rápido e ótimo custo-benefício no almoço executivo.\",\n",
    "    \"Lugar barulhento e comida sem tempero. Não recomendo.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3770dff-c734-45e7-b966-aab5e157bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = [\"para para para para teste meu sol\", \"para para gato subiu no sol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5dee92-4c0b-4b05-a694-28d04fe34981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bag = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9840761-f3e5-47f5-b5c4-fb6ba72bd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['40' 'achei' 'acolhedor' 'almoço' 'altos' 'ambiente' 'apresentados'\n",
      " 'atencioso' 'atendimento' 'barulhento' 'bem' 'benefício' 'cardápio'\n",
      " 'certeza' 'com' 'comi' 'comida' 'cozinha' 'criativo' 'custo' 'de'\n",
      " 'deliciosa' 'demorou' 'do' 'esperei' 'estava' 'executivo' 'foi' 'frio'\n",
      " 'garçom' 'ideal' 'impecável' 'jantar' 'já' 'lugar' 'mais' 'mas' 'melhor'\n",
      " 'minutos' 'muito' 'nenhuma' 'no' 'não' 'opções' 'os' 'para' 'pelo'\n",
      " 'poucas' 'prato' 'pratos' 'preços' 'principal' 'qualidade' 'que'\n",
      " 'recomendo' 'risoto' 'romântico' 'rápido' 'sem' 'servido' 'sobremesa'\n",
      " 'tempero' 'um' 'vegana' 'vegetarianas' 'veio' 'voltarei' 'ótimo']\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "names = vectorizer.get_feature_names_out()\n",
    "print(names)\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69277604-391c-479f-b0b6-b51f69ac1ea7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comida': 16, 'estava': 25, 'deliciosa': 21, 'atendimento': 8, 'foi': 27, 'impecável': 31, 'esperei': 24, 'mais': 35, 'de': 20, '40': 0, 'minutos': 38, 'pelo': 46, 'prato': 48, 'principal': 51, 'veio': 65, 'frio': 28, 'ambiente': 5, 'acolhedor': 2, 'ideal': 30, 'para': 45, 'um': 62, 'jantar': 32, 'romântico': 56, 'achei': 1, 'os': 44, 'preços': 50, 'altos': 4, 'qualidade': 52, 'do': 23, 'que': 53, 'servido': 59, 'melhor': 37, 'risoto': 55, 'já': 33, 'comi': 15, 'voltarei': 66, 'com': 14, 'certeza': 13, 'garçom': 29, 'atencioso': 7, 'mas': 36, 'cozinha': 17, 'demorou': 22, 'muito': 39, 'cardápio': 12, 'criativo': 18, 'pratos': 49, 'bem': 10, 'apresentados': 6, 'poucas': 47, 'opções': 43, 'vegetarianas': 64, 'nenhuma': 40, 'sobremesa': 60, 'vegana': 63, 'rápido': 57, 'ótimo': 67, 'custo': 19, 'benefício': 11, 'no': 41, 'almoço': 3, 'executivo': 26, 'lugar': 34, 'barulhento': 9, 'sem': 58, 'tempero': 61, 'não': 42, 'recomendo': 54}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a98029a-6a3d-4a93-bcf4-7e11fe2facf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 73 stored elements and shape (10, 68)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08921a63-987e-4ef2-91af-736b33978538",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e19d64-7956-48b5-a7ff-97179582a0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 68)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11dbf5-83fd-405c-87b8-f27ace1e6815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "076100a1-0174-45d4-8364-10b23ef0f37f",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e551879-ae5e-4261-b8b2-9e38719a5136",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3034b93b-2da3-40b4-8e65-e8475b730afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato' 'meu' 'no' 'para' 'sol' 'subiu' 'teste']\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tf_idf = TfidfVectorizer(norm=None)\n",
    "tf_idf = vectorizer_tf_idf.fit_transform(toy)\n",
    "names = vectorizer_tf_idf.get_feature_names_out()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "573356ac-8c5d-4d75-8b1e-b8d8b28e91e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'para': 3, 'teste': 6, 'meu': 1, 'sol': 4, 'gato': 0, 'subiu': 5, 'no': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_tf_idf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d8c9bd-a62f-430d-9a8d-ab0b593e3ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.40546511, 0.        , 4.        , 1.        ,\n",
       "        0.        , 1.40546511],\n",
       "       [1.40546511, 0.        , 1.40546511, 2.        , 1.        ,\n",
       "        1.40546511, 0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_words = tf_idf.toarray()\n",
    "tf_idf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703047e6-1fb8-4337-9453-c44cefdd98da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer1 \u001b[38;5;241m=\u001b[39m TfidfVectorizer(norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m tf_idf1 \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m names \u001b[38;5;241m=\u001b[39m vectorizer1\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(names)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \n\u001b[1;32m   2113\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2126\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2128\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(norm=None)\n",
    "tf_idf1 = vectorizer1.transform(toy)\n",
    "names = vectorizer1.get_feature_names_out()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da6009e-8956-4aa1-9590-53d5ce2b617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras do vocabulário: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF do primeiro documento: [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "toy = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "vectorizer1 = TfidfVectorizer(norm=None)\n",
    "vectorizer1.fit(toy)  # aprende os termos\n",
    "\n",
    "tf_idf1 = vectorizer1.transform(toy)  # aplica aos documentos\n",
    "\n",
    "names = vectorizer1.get_feature_names_out()\n",
    "print(\"Palavras do vocabulário:\", names)\n",
    "\n",
    "# Visualiza o TF-IDF de todos os termos no primeiro documento\n",
    "print(\"TF-IDF do primeiro documento:\", tf_idf1[0].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac6799b-206d-4f67-b26f-902f13ebfd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF value for 'document' is: 0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()#usa normalização L2, o que deixa os valores entre 0 e 1\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "\n",
    "document = [\"This is the first document\"]\n",
    "tfidf_matrix = tfidf_vectorizer.transform(document)\n",
    "\n",
    "term = \"document\"\n",
    "term_index = tfidf_vectorizer.vocabulary_[term]\n",
    "\n",
    "tfidf_value = tfidf_matrix[0, term_index]\n",
    "\n",
    "print(f\"The TF-IDF value for '{term}' is: {tfidf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fdd71c-8151-497e-849e-73baa96927f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: TF-IDF('document') = 0.470\n",
      "Doc 2: TF-IDF('document') = 0.688\n",
      "Doc 3: TF-IDF('document') = 0.000\n",
      "Doc 4: TF-IDF('document') = 0.470\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: mini buscador\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "tfidf_vectorizer = TfidfVectorizer()#usa normalização L2, o que deixa os valores entre 0 e 1\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "\n",
    "term = \"document\"\n",
    "term_index = tfidf_vectorizer.vocabulary_[term]\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_value = tfidf_vectorizer.transform([doc])[0, term_index]\n",
    "    print(f\"Doc {i+1}: TF-IDF('{term}') = {tfidf_value:.3f}\")\n",
    "\n",
    "#O documento 2 tem o maior peso para \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4578897-5e06-4884-b36b-beb3209bbe87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuscar_documentos_por_relevancia\u001b[39m(corpus, consulta):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Dado um corpus de documentos e uma string de consulta,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    retorna os documentos ordenados por relevância com base em TF-IDF.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "def buscar_documentos_por_relevancia(corpus, consulta):\n",
    "    \"\"\"\n",
    "    Dado um corpus de documentos e uma string de consulta,\n",
    "    retorna os documentos ordenados por relevância com base em TF-IDF.\n",
    "    \"\"\"\n",
    "    # Cria o vetor TF-IDF com documentos + consulta\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus + [consulta])\n",
    "\n",
    "    # Separa o vetor da consulta (última linha) e os vetores dos documentos\n",
    "    consulta_vector = tfidf_matrix[-1]\n",
    "    document_vectors = tfidf_matrix[:-1]\n",
    "\n",
    "    # Produto escalar como métrica de similaridade (proxy simples de relevância)\n",
    "    similaridades = document_vectors @ consulta_vector.T\n",
    "\n",
    "    # Monta DataFrame com os resultados\n",
    "    ranking_df = pd.DataFrame({\n",
    "        \"Documento\": corpus,\n",
    "        \"Relevância para a consulta\": similaridades.toarray().flatten()\n",
    "    }).sort_values(by=\"Relevância para a consulta\", ascending=False)\n",
    "\n",
    "    return ranking_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Corpus de exemplo (reviews)\n",
    "    corpus = [\n",
    "        \"A comida estava deliciosa e o atendimento foi impecável.\",\n",
    "        \"Esperei mais de 40 minutos pelo prato principal e veio frio.\",\n",
    "        \"O ambiente é acolhedor, ideal para um jantar romântico.\",\n",
    "        \"Achei os preços altos para a qualidade do que foi servido.\",\n",
    "        \"Melhor risoto que já comi, voltarei com certeza!\",\n",
    "        \"Garçom atencioso, mas a cozinha demorou muito.\",\n",
    "        \"Cardápio criativo e pratos bem apresentados.\",\n",
    "        \"Poucas opções vegetarianas e nenhuma sobremesa vegana.\",\n",
    "        \"Atendimento rápido e ótimo custo-benefício no almoço executivo.\",\n",
    "        \"Lugar barulhento e comida sem tempero. Não recomendo.\"\n",
    "    ]\n",
    "\n",
    "    consulta = \"comida fria ruim\"\n",
    "    resultado = buscar_documentos_por_relevancia(corpus, consulta)\n",
    "    print(resultado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_bag_of_words)",
   "language": "python",
   "name": "env_bag_of_words"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
